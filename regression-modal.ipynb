{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Concrete Compressive Strength Using Deep Learning: A Regression Approach with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below.\n",
    "\n",
    "!pip install numpy==2.0.2\n",
    "!pip install pandas==2.2.2\n",
    "!pip install tensorflow_cpu==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='data/concrete_data.csv'\n",
    "concrete_data = pd.read_csv(filepath)\n",
    "\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data_columns = concrete_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = predictors_norm.shape[1] # number of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import Keras Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regression model\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_cols,)))  # Input layer\n",
    "    model.add(Dense(10, activation='relu'))  # One hidden layer with 10 nodes and ReLU activation\n",
    "    model.add(Dense(1))  # Output layer for regression (single output node)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Adam optimizer and MSE loss function\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "baseline_model = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "baseline_model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly split the data into a training and test sets by holding 30% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "baseline_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate():\n",
    "    # Split the data into training and test sets (30% test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=None)\n",
    "    \n",
    "    # Build the model\n",
    "    model = regression_model()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)  # Train for 50 epochs\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the process 50 times\n",
    "mse_list = [train_and_evaluate() for _ in range(50)]\n",
    "\n",
    "# Compute the mean and standard deviation of the MSEs\n",
    "mean_mse = np.mean(mse_list)\n",
    "std_mse = np.std(mse_list)\n",
    "\n",
    "# Report the results\n",
    "print(f\"Mean of Mean Squared Errors: {mean_mse}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Normalize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "predictors_norm = scaler.fit_transform(predictors)  # Normalize predictors\n",
    "\n",
    "# Function to train and evaluate the model on normalized data\n",
    "def train_and_evaluate_normalized():\n",
    "    # Split the data into training and test sets (30% test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=None)\n",
    "    \n",
    "    # Build the model\n",
    "    model = regression_model()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)  # Train for 50 epochs\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return mse\n",
    "\n",
    "# Repeat the process 50 times with normalized data\n",
    "mse_list_normalized = [train_and_evaluate_normalized() for _ in range(50)]\n",
    "\n",
    "# Compute the mean and standard deviation of the MSEs for normalized data\n",
    "mean_mse_normalized = np.mean(mse_list_normalized)\n",
    "std_mse_normalized = np.std(mse_list_normalized)\n",
    "\n",
    "# Report the results\n",
    "print(f\"Mean of Mean Squared Errors (Normalized): {mean_mse_normalized}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors (Normalized): {std_mse_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The normalized data yields a more accurate model with a mean MSE approximately 7.59% lower than the non-normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Increase the number of epochs to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model on normalized data\n",
    "def train_and_evaluate_normalized_100_epochs():\n",
    "    # Split the data into training and test sets (30% test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=None)\n",
    "    \n",
    "    # Build the model\n",
    "    model = regression_model()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)  # Train for 100 epochs\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return mse\n",
    "\n",
    "# Repeat the process 50 times with normalized data and 100 epochs\n",
    "mse_list_normalized_100_epochs = [train_and_evaluate_normalized_100_epochs() for _ in range(50)]\n",
    "\n",
    "# Compute the mean and standard deviation of the MSEs for normalized data\n",
    "mean_mse_normalized_100_epochs = np.mean(mse_list_normalized_100_epochs)\n",
    "std_mse_normalized_100_epochs = np.std(mse_list_normalized_100_epochs)\n",
    "\n",
    "# Report the results\n",
    "print(f\"Mean of Mean Squared Errors (Normalized, 100 Epochs): {mean_mse_normalized_100_epochs}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors (Normalized, 100 Epochs): {std_mse_normalized_100_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the number of epochs from 50 to 100 led to a substantial reduction in mean MSE (approximately 54.18% improvement). This shows that additional training epochs helped the model learn more effectively, but care should be taken to ensure further increases do not lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Increase the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_three_layers():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(predictors_norm.shape[1],)))  # Input layer\n",
    "    model.add(Dense(10, activation='relu'))  # First hidden layer\n",
    "    model.add(Dense(10, activation='relu'))  # Second hidden layer\n",
    "    model.add(Dense(10, activation='relu'))  # Third hidden layer\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    \n",
    "    # Compile the model with the Adam optimizer and mean squared error loss function\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model on normalized data\n",
    "def train_and_evaluate_normalized_100_epochs_three_layers():\n",
    "    # Split the data into training and test sets (30% test set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=None)\n",
    "    \n",
    "    # Build the model\n",
    "    model = regression_model_three_layers()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)  # Train for 100 epochs\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute the mean squared error\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return mse\n",
    "\n",
    "# Repeat the process 50 times with normalized data and 100 epochs\n",
    "mse_list_normalized_100_epochs_three_layers = [train_and_evaluate_normalized_100_epochs_three_layers() for _ in range(50)]\n",
    "\n",
    "# Compute the mean and standard deviation of the MSEs for normalized data\n",
    "mean_mse_normalized_100_epochs_three_layers = np.mean(mse_list_normalized_100_epochs_three_layers)\n",
    "std_mse_normalized_100_epochs_three_layers = np.std(mse_list_normalized_100_epochs_three_layers)\n",
    "\n",
    "# Report the results\n",
    "print(f\"Mean of Mean Squared Errors (Normalized, 100 Epochs, 3 Layers): {mean_mse_normalized_100_epochs_three_layers}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors (Normalized, 100 Epochs, 3 Layers): {std_mse_normalized_100_epochs_three_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Mean Squared Error decreased from 363.039 (50 epochs, 1 layer) to 90.310 (100 epochs, 3 layers), reflecting a 75.12% improvement. This demonstrates that a deeper neural network trained for more epochs can significantly enhance the predictive power of the model in regression tasks. However, monitoring validation metrics is crucial to avoid overfitting with deeper networks and longer training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
